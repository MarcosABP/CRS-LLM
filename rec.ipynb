{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd029aae",
   "metadata": {},
   "source": [
    "Extra√ß√£o(LLM) - Recomenda√ß√£o (RecSys) - Review (LLM) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa855b31",
   "metadata": {},
   "source": [
    "## IMPORTS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ececf9",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b413361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pandas  \n",
    "#!pip install numpy==1.26.4 pandas==2.2.2\n",
    "#!pip install gdown pandas numpy\n",
    "#!pip install kagglehub[pandas-datasets]\n",
    "#!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d57e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai # API openai\n",
    "from tqdm import tqdm\n",
    "import zipfile # Extrair .zip\n",
    "import json # Trabalhar com JSON\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a52e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10006 train conversations\n",
      "Loaded 1342 test conversations\n"
     ]
    }
   ],
   "source": [
    "# Importando dados de treino e teste\n",
    "with zipfile.ZipFile('data/redial_dataset.zip', 'r') as z:\n",
    "    z.extractall('data/')\n",
    "\n",
    "train_data = []\n",
    "for line in open(\"data/train_data.jsonl\", \"r\"):\n",
    "    train_data.append(json.loads(line))\n",
    "print(\"Loaded {} train conversations\".format(len(train_data)))\n",
    "\n",
    "\n",
    "test_data = []\n",
    "for line in open(\"data/test_data.jsonl\", \"r\"):\n",
    "    test_data.append(json.loads(line))\n",
    "print(\"Loaded {} test conversations\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7822e8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movieMentions': {'127328': 'Hairspray  (2007)',\n",
       "  '172129': 'Chicago 10  (2007)',\n",
       "  '124268': 'Mamma Mia!  (2008)',\n",
       "  '145997': 'Chicago  (2002)',\n",
       "  '124461': 'White Christmas  (1954)',\n",
       "  '107350': 'Sweeney Todd: The Demon Barber of Fleet Street  (2007)'},\n",
       " 'respondentQuestions': {'127328': {'suggested': 0, 'seen': 1, 'liked': 1},\n",
       "  '172129': {'suggested': 1, 'seen': 1, 'liked': 1},\n",
       "  '124268': {'suggested': 1, 'seen': 0, 'liked': 1},\n",
       "  '145997': {'suggested': 1, 'seen': 1, 'liked': 1},\n",
       "  '124461': {'suggested': 1, 'seen': 1, 'liked': 1},\n",
       "  '107350': {'suggested': 0, 'seen': 1, 'liked': 1}},\n",
       " 'messages': [{'timeOffset': 0,\n",
       "   'text': 'Hi can you help me find a musical',\n",
       "   'senderWorkerId': 1,\n",
       "   'messageId': 1575},\n",
       "  {'timeOffset': 2,\n",
       "   'text': 'hey what kind of movies interest you',\n",
       "   'senderWorkerId': 14,\n",
       "   'messageId': 1576},\n",
       "  {'timeOffset': 13,\n",
       "   'text': 'yes i would love to!',\n",
       "   'senderWorkerId': 14,\n",
       "   'messageId': 1577},\n",
       "  {'timeOffset': 60,\n",
       "   'text': 'I really like @127328 and @107350',\n",
       "   'senderWorkerId': 1,\n",
       "   'messageId': 1578},\n",
       "  {'timeOffset': 99,\n",
       "   'text': 'have you seen @172129 with Renee Zellwegger, Queen Latifa, and Catherine Zeta Jones',\n",
       "   'senderWorkerId': 14,\n",
       "   'messageId': 1579},\n",
       "  {'timeOffset': 114,\n",
       "   'text': 'I like @127328 too!',\n",
       "   'senderWorkerId': 14,\n",
       "   'messageId': 1580},\n",
       "  {'timeOffset': 134,\n",
       "   'text': 'oops, I meant @145997',\n",
       "   'senderWorkerId': 14,\n",
       "   'messageId': 1581},\n",
       "  {'timeOffset': 145,\n",
       "   'text': 'Yeah I have seen @145997 many times',\n",
       "   'senderWorkerId': 1,\n",
       "   'messageId': 1582},\n",
       "  {'timeOffset': 201,\n",
       "   'text': 'what about @124461 for something a little different or even @124268 which was more recent?',\n",
       "   'senderWorkerId': 14,\n",
       "   'messageId': 1583},\n",
       "  {'timeOffset': 256,\n",
       "   'text': 'I have not seen @124268 I think I may like that one',\n",
       "   'senderWorkerId': 1,\n",
       "   'messageId': 1584},\n",
       "  {'timeOffset': 285,\n",
       "   'text': 'The music is all from ABBA and it is so good',\n",
       "   'senderWorkerId': 14,\n",
       "   'messageId': 1585},\n",
       "  {'timeOffset': 296,\n",
       "   'text': 'I hope you will enjoy it as much as I do',\n",
       "   'senderWorkerId': 14,\n",
       "   'messageId': 1586},\n",
       "  {'timeOffset': 307,\n",
       "   'text': 'That is great that group does have some good music thanks for the help. Bye',\n",
       "   'senderWorkerId': 1,\n",
       "   'messageId': 1587},\n",
       "  {'timeOffset': 315,\n",
       "   'text': 'no problem bye',\n",
       "   'senderWorkerId': 14,\n",
       "   'messageId': 1588}],\n",
       " 'conversationId': '485',\n",
       " 'respondentWorkerId': 14,\n",
       " 'initiatorWorkerId': 1,\n",
       " 'initiatorQuestions': {'127328': {'suggested': 0, 'seen': 1, 'liked': 1},\n",
       "  '172129': {'suggested': 1, 'seen': 1, 'liked': 1},\n",
       "  '124268': {'suggested': 1, 'seen': 0, 'liked': 1},\n",
       "  '145997': {'suggested': 1, 'seen': 1, 'liked': 1},\n",
       "  '124461': {'suggested': 1, 'seen': 1, 'liked': 1},\n",
       "  '107350': {'suggested': 0, 'seen': 1, 'liked': 1}}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7eb66dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Conversa ID: 485 ===\n",
      "Usu√°rio 1: Hi can you help me find a musical\n",
      "Usu√°rio 2: hey what kind of movies interest you\n",
      "Usu√°rio 2: yes i would love to!\n",
      "Usu√°rio 1: I really like @127328 and @107350\n",
      "Usu√°rio 2: have you seen @172129 with Renee Zellwegger, Queen Latifa, and Catherine Zeta Jones\n",
      "Usu√°rio 2: I like @127328 too!\n",
      "Usu√°rio 2: oops, I meant @145997\n",
      "Usu√°rio 1: Yeah I have seen @145997 many times\n",
      "Usu√°rio 2: what about @124461 for something a little different or even @124268 which was more recent?\n",
      "Usu√°rio 1: I have not seen @124268 I think I may like that one\n",
      "Usu√°rio 2: The music is all from ABBA and it is so good\n",
      "Usu√°rio 2: I hope you will enjoy it as much as I do\n",
      "Usu√°rio 1: That is great that group does have some good music thanks for the help. Bye\n",
      "Usu√°rio 2: no problem bye\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de conversa \n",
    "conversation = train_data[32]\n",
    "\n",
    "# Mostrar de forma formatada\n",
    "print(\"=== Conversa ID:\", conversation[\"conversationId\"], \"===\")\n",
    "for msg in conversation[\"messages\"]:\n",
    "    sender = \"Usu√°rio 1\" if msg[\"senderWorkerId\"] == conversation[\"initiatorWorkerId\"] else \"Usu√°rio 2\"\n",
    "    print(f\"{sender}: {msg['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa67f0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Conversa ID: 485 ===\n",
      "Usu√°rio 1: Hi can you help me find a musical\n",
      "Usu√°rio 2: hey what kind of movies interest you\n",
      "Usu√°rio 2: yes i would love to!\n",
      "Usu√°rio 1: I really like Hairspray  (2007) and Sweeney Todd: The Demon Barber of Fleet Street  (2007)\n",
      "Usu√°rio 2: have you seen Chicago 10  (2007) with Renee Zellwegger, Queen Latifa, and Catherine Zeta Jones\n",
      "Usu√°rio 2: I like Hairspray  (2007) too!\n",
      "Usu√°rio 2: oops, I meant Chicago  (2002)\n",
      "Usu√°rio 1: Yeah I have seen Chicago  (2002) many times\n",
      "Usu√°rio 2: what about White Christmas  (1954) for something a little different or even Mamma Mia!  (2008) which was more recent?\n",
      "Usu√°rio 1: I have not seen Mamma Mia!  (2008) I think I may like that one\n",
      "Usu√°rio 2: The music is all from ABBA and it is so good\n",
      "Usu√°rio 2: I hope you will enjoy it as much as I do\n",
      "Usu√°rio 1: That is great that group does have some good music thanks for the help. Bye\n",
      "Usu√°rio 2: no problem bye\n"
     ]
    }
   ],
   "source": [
    "movie_mentions = conversation.get('movieMentions', {})\n",
    "\n",
    "print(f\"=== Conversa ID: {conversation['conversationId']} ===\")\n",
    "\n",
    "for msg in conversation[\"messages\"]:\n",
    "    sender = \"Usu√°rio 1\" if msg[\"senderWorkerId\"] == conversation[\"initiatorWorkerId\"] else \"Usu√°rio 2\"\n",
    "    text = msg['text']\n",
    "    \n",
    "    # Substitui cada @movieId pelo t√≠tulo do filme\n",
    "    for movie_id, movie_title in movie_mentions.items():\n",
    "        text = text.replace(f'@{movie_id}', movie_title)\n",
    "    \n",
    "    print(f\"{sender}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e234418d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID do usu√°rio que iniciou a conversa: 56\n",
      "ID do usu√°rio que respondeu: 60\n"
     ]
    }
   ],
   "source": [
    "conversation = train_data[222]\n",
    "\n",
    "# IDs de usu√°rios\n",
    "initiator_id = conversation[\"initiatorWorkerId\"]\n",
    "respondent_id = conversation[\"respondentWorkerId\"]\n",
    "\n",
    "print(\"ID do usu√°rio que iniciou a conversa:\", initiator_id)\n",
    "print(\"ID do usu√°rio que respondeu:\", respondent_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88c97ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtrar outliers que assistiram muitos ou quase nenhum filmes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362c32ca",
   "metadata": {},
   "source": [
    "## Tratamento e Limpeza dos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "106aa1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conversas v√°lidas restantes: 10005\n",
      "üóëÔ∏è Conversas removidas: 0\n",
      "‚úÖ Conversas v√°lidas restantes: 1342\n",
      "üóëÔ∏è Conversas removidas: 0\n"
     ]
    }
   ],
   "source": [
    "def clean_data(dataset):\n",
    "    \"\"\"\n",
    "    Remove di√°logos inv√°lidas do dataset \n",
    "    \"\"\"\n",
    "\n",
    "    invalid_indexes = []\n",
    "\n",
    "    # Identifica di√°logos inv√°lidas\n",
    "    for i, conv in enumerate(dataset):\n",
    "        if (\n",
    "            isinstance(conv.get(\"movieMentions\"), list) # Se for lista em vez de dicion√°rio\n",
    "            or not isinstance(conv.get(\"movieMentions\", {}), dict) # Se dicion√°rio for inv√°lido\n",
    "            or \"messages\" not in conv # Di√°logo sem mensagem\n",
    "            or not conv[\"messages\"] # Di√°logo sem mensagem\n",
    "        ):\n",
    "            invalid_indexes.append(i)\n",
    "\n",
    "    # Remove invalidos\n",
    "    for i in sorted(invalid_indexes, reverse=True):\n",
    "        del dataset[i]\n",
    "\n",
    "    print(f\"‚úÖ Conversas v√°lidas restantes: {len(dataset)}\")\n",
    "    print(f\"üóëÔ∏è Conversas removidas: {len(invalid_indexes)}\")\n",
    "\n",
    "clean_data(train_data)\n",
    "clean_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7decd453",
   "metadata": {},
   "source": [
    "## Extra√ß√£o de itens e Montagem Matriz Usu√°rio-Item (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac305f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai\n",
    "#!pip install implicit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ecc54",
   "metadata": {},
   "source": [
    "### API LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51f0383a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112233\n",
      "A vida √© feita de altos e baixos, mas o importante √© nunca desistir.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "client_async = AsyncOpenAI(api_key=\"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\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"oi, digite 112233 e uma frase qualquer\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e13547",
   "metadata": {},
   "source": [
    "### Extra√ß√£o Itens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdeb336",
   "metadata": {},
   "source": [
    "#### Fun√ß√µes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3abb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra√ß√£o e atitude de user/item\n",
    "\n",
    "# Formata cada dialogo de JSON para string, separando usu√°rio de sistema \n",
    "def format_conversation(conv):\n",
    "\n",
    "    \"\"\"\n",
    "    Formata cada dialogo de JSON para string, separando usu√°rio de sistema \n",
    "    \"\"\"\n",
    "\n",
    "    msgs = []\n",
    "    for msg in conv[\"messages\"]:\n",
    "        sender = \"User\" if msg[\"senderWorkerId\"] == conv[\"initiatorWorkerId\"] else \"System\"\n",
    "        msgs.append(f\"{sender}: {msg['text']}\")\n",
    "    return \"\\n\".join(msgs)\n",
    "\n",
    "\n",
    "\n",
    "def prompt_extraction_items(conv):\n",
    "\n",
    "    \"\"\"\n",
    "    Gera o prompt que ser√° enviado √† LLM para extrair os filmes e atribuir ratings (1 a 5) apropriadas de acordo com a atitude do usu√°rio.\n",
    "    conversation_text = Dialogo\n",
    "    mentioned_ids = IDs dos filmes mencionados\n",
    "    filtered_mentions = Mantem apenas os IDs que aparecem na conversa \n",
    "    movie_mentions = String com o nome de filmes mencionados\n",
    "    \"\"\"\n",
    "\n",
    "    conversation_text = format_conversation(conv)\n",
    "    mentioned_ids = re.findall(r'@(\\d+)', conversation_text)\n",
    "    filtered_mentions = {k: v for k, v in conv[\"movieMentions\"].items() if k in mentioned_ids}\n",
    "    movie_mentions = \"\\n\".join([f\"@{k} = {v}\" for k, v in filtered_mentions.items()])\n",
    "\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "# Pretend you are a movie recommender system. You (a\n",
    "# recommender system) will be given a full conversation between a user\n",
    "# and a system. Based on the entire conversation, you need to extract ALL \n",
    "# movie names mentioned and analyze the user's final attitude toward each movie.\n",
    "# You need to reply with standardized movie names (with grammatical errors corrected \n",
    "# and abbreviations fixed), as well as the user's attitude toward the movie.\n",
    "\n",
    "\n",
    "# Movies in the conversation are referred to by tokens like \"@12345\". The\n",
    "# user's final attitude toward each movie is represented in one of \n",
    "# [1, 2, 3, 4, 5], where 1 stands for very negative, 2 stands for \n",
    "# negative, 3 stands for neutral, 4 stands for positive, and 5 stands for \n",
    "# very positive. You need to reply with the number as an attitude instead \n",
    "# of the textual description. If there are movie names mentioned in the query,\n",
    "# list each movie name and the user's attitude (number in 0 to\n",
    "# 5) in the form of movie_name####attitude, where different\n",
    "# movies are listed in different lines with no extra sentences.\n",
    "# Reply NO if no movie names are mentioned in the query.\n",
    "\n",
    "# Use the following mapping to know which movie each token refers to:\n",
    "# {movie_mentions}\n",
    "\n",
    "# Important: ONLY consider movies explicitly mentioned (tokens starting with '@'). \n",
    "# Do NOT include any other movies even if related or implied.\n",
    "# If you include any movie not mentioned, your answer will be invalid.\n",
    "\n",
    "# Conversation:\n",
    "# {conversation_text}\n",
    "# \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Pretend you are a movie recommender system. You (a\n",
    "recommender system) will be given a full conversation between a user\n",
    "and a system. Based on the entire conversation, you need to extract ALL \n",
    "movie ids mentioned and analyze the user's final attitude toward each movie.\n",
    "You need to reply with movie ids as well as the user's attitude toward the movie.\n",
    "\n",
    "\n",
    "Movies in the conversation are referred to by tokens like \"@12345\". The\n",
    "user's final attitude toward each movie is represented in one of \n",
    "[1, 2, 3, 4, 5], where 1 stands for very negative, 2 stands for \n",
    "negative, 3 stands for neutral, 4 stands for positive, and 5 stands for \n",
    "very positive. You need to reply with the number as an attitude instead \n",
    "of the textual description. If there are movie ids mentioned in the query,\n",
    "list each movie id and the user's attitude (number in 0 to\n",
    "5) in the form of movie_id####attitude, where different\n",
    "movies are listed in different lines with no extra sentences.\n",
    "You must reply ONLY with movie IDs (e.g., 122159), not movie names.\n",
    "You mus remove the \"@\" that comes in the beginning of the movie id.\n",
    "\n",
    "\n",
    "Use the following to know which movie each token refers to:\n",
    "{movie_mentions}\n",
    "\n",
    "Important: ONLY consider movies explicitly mentioned (tokens starting with '@'). \n",
    "Do NOT include any other movies even if related or implied.\n",
    "If you include any movie not mentioned, your answer will be invalid.\n",
    "\n",
    "Conversation:\n",
    "{conversation_text}\n",
    "\n",
    "Example output format:\n",
    "122159####5\n",
    "84779####4\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "\n",
    "\n",
    "def extract_user_movie_rating(conv, llm_response):\n",
    "\n",
    "    \"\"\"\n",
    "    Extrai userId, movieId e rating a partir da resposta da LLM\n",
    "    \"\"\"\n",
    "\n",
    "    user_id = conv[\"initiatorWorkerId\"]\n",
    "    lines = llm_response.strip().splitlines()\n",
    "    data = []\n",
    "\n",
    "    for line in lines:\n",
    "        if \"####\" in line:\n",
    "            movie_id, rating = line.split(\"####\")\n",
    "\n",
    "            data.append({\n",
    "                \"userId\": user_id,\n",
    "                \"movieId\": int(movie_id.strip()),\n",
    "                \"rating\": int(rating.strip())\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec71484c",
   "metadata": {},
   "source": [
    "#### SYNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b47d2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROMPT ===\n",
      "Pretend you are a movie recommender system. You (a\n",
      "recommender system) will be given a full conversation between a user\n",
      "and a system. Based on the entire conversation, you need to extract ALL \n",
      "movie ids mentioned and analyze the user's final attitude toward each movie.\n",
      "You need to reply with movie ids as well as the user's attitude toward the movie.\n",
      "\n",
      "\n",
      "Movies in the conversation are referred to by tokens like \"@12345\". The\n",
      "user's final attitude toward each movie is represented in one of \n",
      "[1, 2, 3, 4, 5], where 1 stands for very negative, 2 stands for \n",
      "negative, 3 stands for neutral, 4 stands for positive, and 5 stands for \n",
      "very positive. You need to reply with the number as an attitude instead \n",
      "of the textual description. If there are movie ids mentioned in the query,\n",
      "list each movie id and the user's attitude (number in 0 to\n",
      "5) in the form of movie_id####attitude, where different\n",
      "movies are listed in different lines with no extra sentences.\n",
      "You must reply ONLY with movie IDs (e.g., 122159), not movie names.\n",
      "You mus remove the \"@\" that comes in the beginning of the movie id.\n",
      "\n",
      "\n",
      "Use the following mapping to know which movie each token refers to:\n",
      "@100614 = Bad Santa 2 (2016)\n",
      "@122726 = School for Scoundrels  (2006)\n",
      "@199204 = The Alamo  (2004)\n",
      "@112596 = Sling Blade (1996)\n",
      "@110685 = Mr. Woodcock (2007)\n",
      "\n",
      "Important: ONLY consider movies explicitly mentioned (tokens starting with '@'). \n",
      "Do NOT include any other movies even if related or implied.\n",
      "If you include any movie not mentioned, your answer will be invalid.\n",
      "\n",
      "Conversation:\n",
      "User: HI\n",
      "System: Hi can I help you find a movie to watch?\n",
      "User: Yes. I am open to suggestions.\n",
      "System: Do you have a favorite actor or actress?\n",
      "User: Billy Bob Thornton\n",
      "System: He has so many good movies to chose from. @112596 was good and @199204\n",
      "User: I didn't know he was in The Alamo. I might have to check that one out.\n",
      "System: Did you like @112596\n",
      "User: Of course. It's one of my top five favorite movies of all time.\n",
      "System: I think he did really well in that movie. Billy Bob is Davy Crocket in @199204 do you like comedy movies\n",
      "User: Good comedies are rare. I liked Office Space, but that's an old one.\n",
      "User: I forgot to mention Bad Santa. Best comedy ever\n",
      "System: @100614 was pretty good and @110685\n",
      "User: @100614 was okay. I was hoping it would be better.\n",
      "System: another really good comedy he is in is @122726\n",
      "User: You're doing some nice Google magic in this chat.\n",
      "System: I have seen most of his movies\n",
      "User: I guess I need to look into more of his movies and see what I missed.\n",
      "System: Have you seen @110685 or @122726\n",
      "User: I saw @110685, but not sure about the other.\n",
      "System: Those two are real funny movies you may like them\n",
      "User: Alright. I'm going to take off now. Thanks.\n",
      "System: Glad I could help. Bye\n",
      "\n",
      "Example output format:\n",
      "122159####5\n",
      "84779####4\n",
      "\n",
      "=== RESPOSTA ===\n",
      "112596####5  \n",
      "199204####4  \n",
      "100614####3  \n",
      "110685####4  \n",
      "122726####3\n",
      "   userId  movieId  rating\n",
      "0      18   112596       5\n",
      "1      18   199204       4\n",
      "2      18   100614       3\n",
      "3      18   110685       4\n",
      "4      18   122726       3\n"
     ]
    }
   ],
   "source": [
    "# Testar com uma conversa\n",
    "conv_35 = train_data[35]\n",
    "\n",
    "# Montagem prompt extra√ß√£o item/rating\n",
    "prompt = prompt_extraction_items(conv_35)\n",
    "\n",
    "# Resposta LLM\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"=== PROMPT ===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n=== RESPOSTA ===\")\n",
    "print(response.choices[0].message.content.strip())\n",
    "\n",
    "\n",
    "# Exemplo com resposta exemplo\n",
    "llm_response = response.choices[0].message.content.strip()\n",
    "\n",
    "# Extrai user/item/rating\n",
    "df_result = extract_user_movie_rating(conv_35, llm_response)\n",
    "\n",
    "print(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f3977",
   "metadata": {},
   "source": [
    "#### ASYNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd83402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execu√ß√£o Ass√≠ncrona\n",
    "\n",
    "async def process_conversation(conv):\n",
    "\n",
    "    \"\"\"\n",
    "    Processa um √∫nico di√°logo\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = prompt_extraction_items(conv) # Gera prompt\n",
    "\n",
    "    for attempt in range(3):  # tenta at√© 3 vezes\n",
    "        try:\n",
    "            # Chamada LLM\n",
    "            response = await client_async.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            # Resosta LLM\n",
    "            content = response.choices[0].message.content.strip()\n",
    "            if content != \"NO\":\n",
    "                return extract_user_movie_rating(conv, content) # Input para matriz usu√°rio item com ratings\n",
    "            break\n",
    "        except Exception as e:\n",
    "            await asyncio.sleep(2) \n",
    "    return pd.DataFrame(columns=[\"userId\", \"movieId\", \"rating\"]) # Fallback\n",
    "\n",
    "\n",
    "async def process_dataset(sample_data, batch_size=10):\n",
    "    \"\"\"\n",
    "    Processa v√°rios di√°logos em paralelo (batch)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i in range(0, len(sample_data), batch_size): # Divide dataset em batches\n",
    "        batch = sample_data[i:i+batch_size]\n",
    "        tasks = [process_conversation(conv) for conv in batch] # Para cada di√°logo no batch, calcula input da matriz\n",
    "        batch_results = await asyncio.gather(*tasks) # Garante execu√ß√£o em paralelo \n",
    "        await asyncio.sleep(2) # evita timeout\n",
    "        results.extend(batch_results)\n",
    "        print(f\" Processados {i+len(batch)} di√°logos\")\n",
    "\n",
    "    df_all = pd.concat(results, ignore_index=True)\n",
    "    return df_all #Matriz usu√°rio item com ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c86261c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processados 40 di√°logos\n",
      " Processados 80 di√°logos\n",
      " Processados 120 di√°logos\n",
      " Processados 160 di√°logos\n",
      " Processados 200 di√°logos\n",
      " Processados 240 di√°logos\n",
      " Processados 280 di√°logos\n",
      " Processados 320 di√°logos\n",
      " Processados 360 di√°logos\n",
      " Processados 400 di√°logos\n",
      " Processados 440 di√°logos\n",
      " Processados 480 di√°logos\n",
      " Processados 520 di√°logos\n",
      " Processados 560 di√°logos\n",
      " Processados 600 di√°logos\n",
      " Processados 640 di√°logos\n",
      " Processados 680 di√°logos\n",
      " Processados 720 di√°logos\n",
      " Processados 760 di√°logos\n",
      " Processados 800 di√°logos\n",
      " Processados 840 di√°logos\n",
      " Processados 880 di√°logos\n",
      " Processados 920 di√°logos\n",
      " Processados 960 di√°logos\n",
      " Processados 1000 di√°logos\n",
      " Processados 1040 di√°logos\n",
      " Processados 1080 di√°logos\n",
      " Processados 1120 di√°logos\n",
      " Processados 1160 di√°logos\n",
      " Processados 1200 di√°logos\n",
      " Processados 1240 di√°logos\n",
      " Processados 1280 di√°logos\n",
      " Processados 1320 di√°logos\n",
      " Processados 1360 di√°logos\n",
      " Processados 1400 di√°logos\n",
      " Processados 1440 di√°logos\n",
      " Processados 1480 di√°logos\n",
      " Processados 1520 di√°logos\n",
      " Processados 1560 di√°logos\n",
      " Processados 1600 di√°logos\n",
      " Processados 1640 di√°logos\n",
      " Processados 1680 di√°logos\n",
      " Processados 1720 di√°logos\n",
      " Processados 1760 di√°logos\n",
      " Processados 1800 di√°logos\n",
      " Processados 1840 di√°logos\n",
      " Processados 1880 di√°logos\n",
      " Processados 1920 di√°logos\n",
      " Processados 1960 di√°logos\n",
      " Processados 2000 di√°logos\n",
      " Processados 2040 di√°logos\n",
      " Processados 2080 di√°logos\n",
      " Processados 2120 di√°logos\n",
      " Processados 2160 di√°logos\n",
      " Processados 2200 di√°logos\n",
      " Processados 2240 di√°logos\n",
      " Processados 2280 di√°logos\n",
      " Processados 2320 di√°logos\n",
      " Processados 2360 di√°logos\n",
      " Processados 2400 di√°logos\n",
      " Processados 2440 di√°logos\n",
      " Processados 2480 di√°logos\n",
      " Processados 2520 di√°logos\n",
      " Processados 2560 di√°logos\n",
      " Processados 2600 di√°logos\n",
      " Processados 2640 di√°logos\n",
      " Processados 2680 di√°logos\n",
      " Processados 2720 di√°logos\n",
      " Processados 2760 di√°logos\n",
      " Processados 2800 di√°logos\n",
      " Processados 2840 di√°logos\n",
      " Processados 2880 di√°logos\n",
      " Processados 2920 di√°logos\n",
      " Processados 2960 di√°logos\n",
      " Processados 3000 di√°logos\n",
      " Processados 3040 di√°logos\n",
      " Processados 3080 di√°logos\n",
      " Processados 3120 di√°logos\n",
      " Processados 3160 di√°logos\n",
      " Processados 3200 di√°logos\n",
      " Processados 3240 di√°logos\n",
      " Processados 3280 di√°logos\n",
      " Processados 3320 di√°logos\n",
      " Processados 3360 di√°logos\n",
      " Processados 3400 di√°logos\n",
      " Processados 3440 di√°logos\n",
      " Processados 3480 di√°logos\n",
      " Processados 3520 di√°logos\n",
      " Processados 3560 di√°logos\n",
      " Processados 3600 di√°logos\n",
      " Processados 3640 di√°logos\n",
      " Processados 3680 di√°logos\n",
      " Processados 3720 di√°logos\n",
      " Processados 3760 di√°logos\n",
      " Processados 3800 di√°logos\n",
      " Processados 3840 di√°logos\n",
      " Processados 3880 di√°logos\n",
      " Processados 3920 di√°logos\n",
      " Processados 3960 di√°logos\n",
      " Processados 4000 di√°logos\n",
      " Processados 4040 di√°logos\n",
      " Processados 4080 di√°logos\n",
      " Processados 4120 di√°logos\n",
      " Processados 4160 di√°logos\n",
      " Processados 4200 di√°logos\n",
      " Processados 4240 di√°logos\n",
      " Processados 4280 di√°logos\n",
      " Processados 4320 di√°logos\n",
      " Processados 4360 di√°logos\n",
      " Processados 4400 di√°logos\n",
      " Processados 4440 di√°logos\n",
      " Processados 4480 di√°logos\n",
      " Processados 4520 di√°logos\n",
      " Processados 4560 di√°logos\n",
      " Processados 4600 di√°logos\n",
      " Processados 4640 di√°logos\n",
      " Processados 4680 di√°logos\n",
      " Processados 4720 di√°logos\n",
      " Processados 4760 di√°logos\n",
      " Processados 4800 di√°logos\n",
      " Processados 4840 di√°logos\n",
      " Processados 4880 di√°logos\n",
      " Processados 4920 di√°logos\n",
      " Processados 4960 di√°logos\n",
      " Processados 5000 di√°logos\n",
      " Processados 5040 di√°logos\n",
      " Processados 5080 di√°logos\n",
      " Processados 5120 di√°logos\n",
      " Processados 5160 di√°logos\n",
      " Processados 5200 di√°logos\n",
      " Processados 5240 di√°logos\n",
      " Processados 5280 di√°logos\n",
      " Processados 5320 di√°logos\n",
      " Processados 5360 di√°logos\n",
      " Processados 5400 di√°logos\n",
      " Processados 5440 di√°logos\n",
      " Processados 5480 di√°logos\n",
      " Processados 5520 di√°logos\n",
      " Processados 5560 di√°logos\n",
      " Processados 5600 di√°logos\n",
      " Processados 5640 di√°logos\n",
      " Processados 5680 di√°logos\n",
      " Processados 5720 di√°logos\n",
      " Processados 5760 di√°logos\n",
      " Processados 5800 di√°logos\n",
      " Processados 5840 di√°logos\n",
      " Processados 5880 di√°logos\n",
      " Processados 5920 di√°logos\n",
      " Processados 5960 di√°logos\n",
      " Processados 6000 di√°logos\n",
      " Processados 6040 di√°logos\n",
      " Processados 6080 di√°logos\n",
      " Processados 6120 di√°logos\n",
      " Processados 6160 di√°logos\n",
      " Processados 6200 di√°logos\n",
      " Processados 6240 di√°logos\n",
      " Processados 6280 di√°logos\n",
      " Processados 6320 di√°logos\n",
      " Processados 6360 di√°logos\n",
      " Processados 6400 di√°logos\n",
      " Processados 6440 di√°logos\n",
      " Processados 6480 di√°logos\n",
      " Processados 6520 di√°logos\n",
      " Processados 6560 di√°logos\n",
      " Processados 6600 di√°logos\n",
      " Processados 6640 di√°logos\n",
      " Processados 6680 di√°logos\n",
      " Processados 6720 di√°logos\n",
      " Processados 6760 di√°logos\n",
      " Processados 6800 di√°logos\n",
      " Processados 6840 di√°logos\n",
      " Processados 6880 di√°logos\n",
      " Processados 6920 di√°logos\n",
      " Processados 6960 di√°logos\n",
      " Processados 7000 di√°logos\n",
      " Processados 7040 di√°logos\n",
      " Processados 7080 di√°logos\n",
      " Processados 7120 di√°logos\n",
      " Processados 7160 di√°logos\n",
      " Processados 7200 di√°logos\n",
      " Processados 7240 di√°logos\n",
      " Processados 7280 di√°logos\n",
      " Processados 7320 di√°logos\n",
      " Processados 7360 di√°logos\n",
      " Processados 7400 di√°logos\n",
      " Processados 7440 di√°logos\n",
      " Processados 7480 di√°logos\n",
      " Processados 7520 di√°logos\n",
      " Processados 7560 di√°logos\n",
      " Processados 7600 di√°logos\n",
      " Processados 7640 di√°logos\n",
      " Processados 7680 di√°logos\n",
      " Processados 7720 di√°logos\n",
      " Processados 7760 di√°logos\n",
      " Processados 7800 di√°logos\n",
      " Processados 7840 di√°logos\n",
      " Processados 7880 di√°logos\n",
      " Processados 7920 di√°logos\n",
      " Processados 7960 di√°logos\n",
      " Processados 8000 di√°logos\n",
      " Processados 8040 di√°logos\n",
      " Processados 8080 di√°logos\n",
      " Processados 8120 di√°logos\n",
      " Processados 8160 di√°logos\n",
      " Processados 8200 di√°logos\n",
      " Processados 8240 di√°logos\n",
      " Processados 8280 di√°logos\n",
      " Processados 8320 di√°logos\n",
      " Processados 8360 di√°logos\n",
      " Processados 8400 di√°logos\n",
      " Processados 8440 di√°logos\n",
      " Processados 8480 di√°logos\n",
      " Processados 8520 di√°logos\n",
      " Processados 8560 di√°logos\n",
      " Processados 8600 di√°logos\n",
      " Processados 8640 di√°logos\n",
      " Processados 8680 di√°logos\n",
      " Processados 8720 di√°logos\n",
      " Processados 8760 di√°logos\n",
      " Processados 8800 di√°logos\n",
      " Processados 8840 di√°logos\n",
      " Processados 8880 di√°logos\n",
      " Processados 8920 di√°logos\n",
      " Processados 8960 di√°logos\n",
      " Processados 9000 di√°logos\n",
      " Processados 9040 di√°logos\n",
      " Processados 9080 di√°logos\n",
      " Processados 9120 di√°logos\n",
      " Processados 9160 di√°logos\n",
      " Processados 9200 di√°logos\n",
      " Processados 9240 di√°logos\n",
      " Processados 9280 di√°logos\n",
      " Processados 9320 di√°logos\n",
      " Processados 9360 di√°logos\n",
      " Processados 9400 di√°logos\n",
      " Processados 9440 di√°logos\n",
      " Processados 9480 di√°logos\n",
      " Processados 9520 di√°logos\n",
      " Processados 9560 di√°logos\n",
      " Processados 9600 di√°logos\n",
      " Processados 9640 di√°logos\n",
      " Processados 9680 di√°logos\n",
      " Processados 9720 di√°logos\n",
      " Processados 9760 di√°logos\n",
      " Processados 9800 di√°logos\n",
      " Processados 9840 di√°logos\n",
      " Processados 9880 di√°logos\n",
      " Processados 9920 di√°logos\n",
      " Processados 9960 di√°logos\n",
      " Processados 10000 di√°logos\n",
      " Processados 10005 di√°logos\n"
     ]
    }
   ],
   "source": [
    "user_item_train = await process_dataset(train_data, batch_size=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b23878f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>84779</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>191602</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>122159</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>151313</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>203371</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31814</th>\n",
       "      <td>953</td>\n",
       "      <td>204974</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31815</th>\n",
       "      <td>954</td>\n",
       "      <td>85036</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31816</th>\n",
       "      <td>954</td>\n",
       "      <td>170277</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31817</th>\n",
       "      <td>954</td>\n",
       "      <td>149938</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31818</th>\n",
       "      <td>954</td>\n",
       "      <td>200018</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31819 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      userId movieId rating\n",
       "0          0   84779      4\n",
       "1          0  191602      4\n",
       "2          0  122159      4\n",
       "3          0  151313      4\n",
       "4          0  203371      4\n",
       "...      ...     ...    ...\n",
       "31814    953  204974      5\n",
       "31815    954   85036      4\n",
       "31816    954  170277      4\n",
       "31817    954  149938      3\n",
       "31818    954  200018      5\n",
       "\n",
       "[31819 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item_train.to_csv(\"user_item_train.csv\", index=False, sep=\";\")\n",
    "user_item_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "480d5603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processados 30 di√°logos\n",
      " Processados 60 di√°logos\n",
      " Processados 90 di√°logos\n",
      " Processados 120 di√°logos\n",
      " Processados 150 di√°logos\n",
      " Processados 180 di√°logos\n",
      " Processados 210 di√°logos\n",
      " Processados 240 di√°logos\n",
      " Processados 270 di√°logos\n",
      " Processados 300 di√°logos\n",
      " Processados 330 di√°logos\n",
      " Processados 360 di√°logos\n",
      " Processados 390 di√°logos\n",
      " Processados 420 di√°logos\n",
      " Processados 450 di√°logos\n",
      " Processados 480 di√°logos\n",
      " Processados 510 di√°logos\n",
      " Processados 540 di√°logos\n",
      " Processados 570 di√°logos\n",
      " Processados 600 di√°logos\n",
      " Processados 630 di√°logos\n",
      " Processados 660 di√°logos\n",
      " Processados 690 di√°logos\n",
      " Processados 720 di√°logos\n",
      " Processados 750 di√°logos\n",
      " Processados 780 di√°logos\n",
      " Processados 810 di√°logos\n",
      " Processados 840 di√°logos\n",
      " Processados 870 di√°logos\n",
      " Processados 900 di√°logos\n",
      " Processados 930 di√°logos\n",
      " Processados 960 di√°logos\n",
      " Processados 990 di√°logos\n",
      " Processados 1020 di√°logos\n",
      " Processados 1050 di√°logos\n",
      " Processados 1080 di√°logos\n",
      " Processados 1110 di√°logos\n",
      " Processados 1140 di√°logos\n",
      " Processados 1170 di√°logos\n",
      " Processados 1200 di√°logos\n",
      " Processados 1230 di√°logos\n",
      " Processados 1260 di√°logos\n",
      " Processados 1290 di√°logos\n",
      " Processados 1320 di√°logos\n",
      " Processados 1342 di√°logos\n"
     ]
    }
   ],
   "source": [
    "user_item_test = await process_dataset(test_data, batch_size=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1094a570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>956</td>\n",
       "      <td>111776</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>956</td>\n",
       "      <td>151656</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>956</td>\n",
       "      <td>192131</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>956</td>\n",
       "      <td>134643</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>956</td>\n",
       "      <td>94688</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>1084</td>\n",
       "      <td>204870</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>1084</td>\n",
       "      <td>95785</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>1084</td>\n",
       "      <td>205724</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>1084</td>\n",
       "      <td>90248</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>1084</td>\n",
       "      <td>177387</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1726 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     userId movieId rating\n",
       "0       956  111776      3\n",
       "1       956  151656      4\n",
       "2       956  192131      3\n",
       "3       956  134643      3\n",
       "4       956   94688      4\n",
       "...     ...     ...    ...\n",
       "1721   1084  204870      4\n",
       "1722   1084   95785      4\n",
       "1723   1084  205724      4\n",
       "1724   1084   90248      5\n",
       "1725   1084  177387      3\n",
       "\n",
       "[1726 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item_test.to_csv(\"user_item_test.csv\", index=False, sep=\";\")\n",
    "user_item_test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c20db9",
   "metadata": {},
   "source": [
    "### Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cccc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "\n",
    "def create_sparse_matrix(df, user_mapper, movie_mapper, shape):\n",
    "    \"\"\"Cria matriz esparsa a partir de DataFrame\"\"\"\n",
    "    df_mapped = df.copy()\n",
    "    df_mapped['user_index'] = df_mapped['userId'].map(user_mapper)#.astype('Int64')\n",
    "    df_mapped['movie_index'] = df_mapped['movieId'].map(movie_mapper)#.astype('Int64')\n",
    "    \n",
    "    # remove linhas com NaN em √≠ndices ou rating inv√°lido\n",
    "    df_mapped = df_mapped.dropna(subset=['user_index', 'movie_index', 'rating'])\n",
    "    df_mapped = df_mapped[df_mapped['rating'] > 0]\n",
    "\n",
    "    # Remove duplicatas\n",
    "    df_mapped = df_mapped.drop_duplicates(subset=['user_index', 'movie_index'], keep='last')\n",
    "    \n",
    "    # Cria matriz esparsa\n",
    "    matrix = csr_matrix(\n",
    "        (df_mapped['rating'].astype(float),\n",
    "         (df_mapped['user_index'], #.astype('Int64')\n",
    "          df_mapped['movie_index'])),#.astype('Int64')\n",
    "        shape=shape\n",
    "    )\n",
    "    return matrix\n",
    "\n",
    "def split_train_validation(user_item_matrix, validation_ratio=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Divide uma matriz esparsa user-item em treino e valida√ß√£o.\n",
    "    Retorna duas matrizes CSR (train, validation)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    coo = user_item_matrix.tocoo()\n",
    "\n",
    "    # embaralha √≠ndices\n",
    "    indices = np.arange(len(coo.data))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split = int(len(indices) * (1 - validation_ratio))\n",
    "    train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "    train = csr_matrix((coo.data[train_idx], (coo.row[train_idx], coo.col[train_idx])),\n",
    "                       shape=coo.shape)\n",
    "    val = csr_matrix((coo.data[val_idx], (coo.row[val_idx], coo.col[val_idx])),\n",
    "                     shape=coo.shape)\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4bc5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 19268 stored elements and shape (594, 4922)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Utility matrix (user-item)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "all_data = pd.concat([user_item_train, user_item_test]) # Concatena train + test para garantir que todos os IDs estejam mapeados\n",
    "\n",
    "user_mapper = {u: i for i, u in enumerate(all_data['userId'].unique())}\n",
    "movie_mapper = {m: i for i, m in enumerate(all_data['movieId'].unique())}\n",
    "\n",
    "n_users = len(user_mapper)\n",
    "n_movies = len(movie_mapper)\n",
    "\n",
    "\n",
    "\n",
    "train_matrix = create_sparse_matrix(user_item_train, user_mapper, movie_mapper, (n_users, n_movies))\n",
    "test_matrix = create_sparse_matrix(user_item_test, user_mapper, movie_mapper, (n_users, n_movies))\n",
    "\n",
    "#Divide treino em treino/valida√ß√£o\n",
    "train_final_matrix, val_matrix = split_train_validation(train_matrix, validation_ratio=0.2, seed=42)\n",
    "\n",
    "train_final_matrix\n",
    "\n",
    "\n",
    "## Estrutura Final\n",
    "# Dados Originais\n",
    "#     ‚Üì\n",
    "# ‚îú‚îÄ‚îÄ user_item_train (80%)\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ train_final_matrix (64% do total)  ‚Üê Treina modelo aqui\n",
    "# ‚îÇ   ‚îî‚îÄ‚îÄ val_matrix (16% do total)          ‚Üê Ajusta hiperpar√¢metros aqui\n",
    "# ‚îÇ\n",
    "# ‚îî‚îÄ‚îÄ user_item_test (20%)\n",
    "#     ‚îî‚îÄ‚îÄ test_matrix (20% do total)         ‚Üê Avalia√ß√£o final aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dce0b5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Movie_0  Movie_1  Movie_2  Movie_3  Movie_4  Movie_5\n",
      "User_0      4.0      4.0      4.0      4.0      4.0      3.0\n",
      "User_1      0.0      0.0      0.0      0.0      0.0      0.0\n",
      "User_2      0.0      0.0      0.0      0.0      0.0      0.0\n",
      "User_3      0.0      0.0      0.0      0.0      0.0      0.0\n",
      "User_4      0.0      0.0      0.0      0.0      0.0      5.0\n",
      "User_5      0.0      0.0      0.0      0.0      0.0      0.0\n",
      "User_6      0.0      0.0      0.0      0.0      0.0      0.0\n",
      "User_7      0.0      0.0      0.0      0.0      0.0      0.0\n",
      "User_8      0.0      0.0      0.0      0.0      0.0      0.0\n",
      "User_9      0.0      0.0      0.0      0.0      0.0      0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Limitar apenas aos primeiros 10 usu√°rios e 10 filmes\n",
    "small_matrix = train_final_matrix[:10, :6].toarray()\n",
    "\n",
    "# Converter para DataFrame s√≥ para visualizar melhor\n",
    "df_view = pd.DataFrame(small_matrix, columns=[f\"Movie_{i}\" for i in range(6)])\n",
    "df_view.index = [f\"User_{i}\" for i in range(10)]\n",
    "\n",
    "print(df_view)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d43b541",
   "metadata": {},
   "source": [
    "### Normaliza√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54371a43",
   "metadata": {},
   "source": [
    "## Treinamento Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c73295b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.recommendation import ALS\n",
    "\n",
    "#!pip install cmake ninja\n",
    "#!pip install --no-cache-dir implicit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df72240e",
   "metadata": {},
   "source": [
    "#### ALS (Alternating Least Squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87793286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prude\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#csr_matrix\n",
    "#Split train validation\n",
    "#Grid search\n",
    "#Model\n",
    "#Recall@k\n",
    "\n",
    "\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "\n",
    "def get_implicit_als(dataset, regularization, alpha_parameter, iterations, factors, random_state):\n",
    "    \"\"\"Gets results from ALS algorithm implementation from implicit library.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset:\n",
    "        Training dataset.\n",
    "    regularization:\n",
    "        Regularization parameter (used for creating regularization term).\n",
    "    alpha_parameter:\n",
    "        Alpha parameter (used for creating confidence matrix).\n",
    "    iterations:\n",
    "        Number of iterations.\n",
    "    factors:\n",
    "        Number of user and item factors.\n",
    "    random_state:\n",
    "        Random state for feature vector initialization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        user_matrix, item_matrix: Matrices with feature vectors for users and items.\n",
    "    \"\"\"\n",
    "    model = AlternatingLeastSquares(factors=factors,\n",
    "                                    regularization=regularization,\n",
    "                                    iterations=iterations,\n",
    "                                    use_native=False)\n",
    "    model.fit((dataset * alpha_parameter).astype('double'))\n",
    "\n",
    "    return model.item_factors, model.user_factors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b950fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04829f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4e023d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_mapped' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Verifica se h√° duplicatas\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m duplicatas = \u001b[43mdf_mapped\u001b[49m.duplicated(subset=[\u001b[33m'\u001b[39m\u001b[33muserId\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmovieId\u001b[39m\u001b[33m'\u001b[39m]).sum()\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mN√∫mero de duplicatas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduplicatas\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Ou compare o tamanho\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_mapped' is not defined"
     ]
    }
   ],
   "source": [
    "# Verifica se h√° duplicatas\n",
    "duplicatas = df_mapped.duplicated(subset=['userId', 'movieId']).sum()\n",
    "print(f\"N√∫mero de duplicatas: {duplicatas}\")\n",
    "\n",
    "# Ou compare o tamanho\n",
    "print(f\"Linhas no df: {len(df_mapped)}\")\n",
    "print(f\"Valores n√£o-zero na matriz: {user_item_matrix.nnz}\")\n",
    "# Se forem iguais, n√£o h√° duplicatas sendo somadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa260009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filmes avaliados m√∫ltiplas vezes pelo usu√°rio 0:\n",
      "movie_index\n",
      "19     2\n",
      "26     2\n",
      "110    2\n",
      "187    2\n",
      "191    2\n",
      "209    2\n",
      "226    2\n",
      "445    3\n",
      "544    2\n",
      "850    2\n",
      "911    2\n",
      "dtype: int64\n",
      "\n",
      "Detalhes das avalia√ß√µes duplicadas:\n",
      "\n",
      "--- Filme 19 ---\n",
      "      userId  movieId  rating  user_index  movie_index\n",
      "1903      88   115908       4          54           19\n",
      "7781      88   115908       4          54           19\n",
      "\n",
      "--- Filme 26 ---\n",
      "      userId  movieId  rating  user_index  movie_index\n",
      "1902      88   204331       4          54           26\n",
      "2114      88   204331       4          54           26\n",
      "\n",
      "--- Filme 110 ---\n",
      "      userId  movieId  rating  user_index  movie_index\n",
      "1885      88   183056       5          54          110\n",
      "1971      88   183056       5          54          110\n",
      "\n",
      "--- Filme 187 ---\n",
      "      userId  movieId  rating  user_index  movie_index\n",
      "1787      88   100493       3          54          187\n",
      "1898      88   100493       4          54          187\n",
      "\n",
      "--- Filme 191 ---\n",
      "      userId  movieId  rating  user_index  movie_index\n",
      "2007      88   201259       4          54          191\n",
      "2040      88   201259       5          54          191\n",
      "\n",
      "--- Filme 209 ---\n",
      "      userId  movieId  rating  user_index  movie_index\n",
      "1789      88   140066       3          54          209\n",
      "1967      88   140066       4          54          209\n",
      "\n",
      "--- Filme 226 ---\n",
      "      userId  movieId  rating  user_index  movie_index\n",
      "1887      88    83552       4          54          226\n",
      "6564      88    83552       5          54          226\n",
      "\n",
      "--- Filme 445 ---\n",
      "      userId  movieId  rating  user_index  movie_index\n",
      "1790      88    84273       3          54          445\n",
      "1883      88    84273       5          54          445\n",
      "2039      88    84273       3          54          445\n",
      "\n",
      "--- Filme 544 ---\n",
      "      userId  movieId  rating  user_index  movie_index\n",
      "1884      88   190814       5          54          544\n",
      "1968      88   190814       3          54          544\n",
      "\n",
      "--- Filme 850 ---\n",
      "      userId  movieId  rating  user_index  movie_index\n",
      "1792      88   143623       5          54          850\n",
      "1969      88   143623       3          54          850\n",
      "\n",
      "--- Filme 911 ---\n",
      "      userId  movieId  rating  user_index  movie_index\n",
      "2006      88   132725       4          54          911\n",
      "2043      88   132725       2          54          911\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d04bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "import numpy as np\n",
    "\n",
    "def recall_at_k(model, train_matrix, test_matrix, K=10):\n",
    "    \"\"\"Calcula Recall@K para as matrizes j√° montadas.\"\"\"\n",
    "    n_users, _ = train_matrix.shape\n",
    "    recalls = []\n",
    "    \n",
    "    for user in range(n_users):\n",
    "        seen_items = train_matrix[user].indices\n",
    "        scores = model.user_factors[user].dot(model.item_factors.T)\n",
    "        scores[seen_items] = -np.inf  # ignora itens j√° vistos\n",
    "        top_k = np.argpartition(scores, -K)[-K:]\n",
    "        \n",
    "        test_items = test_matrix[user].indices\n",
    "        if len(test_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        hits = np.isin(top_k, test_items).sum()\n",
    "        recalls.append(hits / len(test_items))\n",
    "    \n",
    "    return np.mean(recalls)\n",
    "\n",
    "# ----- Grid Search -----\n",
    "\n",
    "param_grid = {\n",
    "    'factors': [50, 100, 150],\n",
    "    'regularization': [0.01, 0.1, 1.0],\n",
    "    'iterations': [10, 20],\n",
    "    'alpha': [10, 40]\n",
    "}\n",
    "\n",
    "best_score = -1\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Treinando com par√¢metros: {params}\")\n",
    "    \n",
    "    model = AlternatingLeastSquares(\n",
    "        factors=params['factors'],\n",
    "        regularization=params['regularization'],\n",
    "        iterations=params['iterations'],\n",
    "        use_gpu=False\n",
    "    )\n",
    "    \n",
    "    model.fit((user_item_train * params['alpha']).astype('double'))\n",
    "    score = recall_at_k(model, user_item_train, user_item_test, K=10)\n",
    "    \n",
    "    print(f\"Recall@10 = {score:.4f}\")\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nMelhores par√¢metros encontrados:\")\n",
    "print(best_params)\n",
    "print(f\"Melhor Recall@10: {best_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff810978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(userCol = \"userId\", itemCol = \"movieId\", labelCol = \"rating\", coldStartStrategy = \"drop\", nonegative = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5b558b",
   "metadata": {},
   "source": [
    "#### BPR (Bayesian Personalized Ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit.bpr import BayesianPersonalizedRanking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
